{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianEyebrow/CSCI_167/blob/FinalProject/167_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iC0jBCydeDF"
      },
      "outputs": [],
      "source": [
        "# Installations\n",
        "!pip install -q torch torchvision pandas\n",
        "# jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --to notebook filename.ipynb\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# For plots\n",
        "%matplotlib inline\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# Load base training dataset with NO transform yet\n",
        "base_train_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None  # we'll add transforms in a wrapper\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "# Train/validation split\n",
        "num_train = len(base_train_dataset)\n",
        "val_size = int(0.1 * num_train)\n",
        "train_size = num_train - val_size\n",
        "\n",
        "indices = np.random.permutation(num_train)\n",
        "val_indices = indices[:val_size]\n",
        "train_indices = indices[val_size:]\n",
        "\n",
        "class CIFAR10Subset(Dataset):\n",
        "    \"\"\"\n",
        "    A thin wrapper to:\n",
        "    - select a subset of indices from a base CIFAR-10 dataset\n",
        "    - apply a given transform to the images\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset, indices, transform=None):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = list(indices)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = self.indices[idx]\n",
        "        img, label = self.base_dataset[base_idx]  # img is PIL image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Create train/val subsets that share the same data but use different transforms\n",
        "train_dataset = CIFAR10Subset(base_train_dataset, train_indices, transform=train_transform)\n",
        "val_dataset   = CIFAR10Subset(base_train_dataset, val_indices,   transform=test_transform)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def make_dataloaders(batch_size=128):\n",
        "    # Avoids multiprocessing issues\n",
        "    num_workers = 0\n",
        "\n",
        "    pin_memory = (device.type == \"cuda\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "TZXuuVL1dhFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple and Deep CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A small CNN for CIFAR-10:\n",
        "    - 2 convolutional layers\n",
        "    - 2 fully-connected layers\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B, 32, 32, 32]\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                             # [B, 32, 16, 16]\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # [B, 64, 16, 16]\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)                              # [B, 64, 8, 8]\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeeperCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A deeper CNN with:\n",
        "    - 3 convolutional blocks\n",
        "    - BatchNorm\n",
        "    - Dropout for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(256 * 4 * 4, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wRZiQpJkdseb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluations\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train_one_epoch(model, optimizer, train_loader, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, leave=False):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = labels.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        running_acc += accuracy_from_logits(outputs, labels) * batch_size\n",
        "        total += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_acc / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        batch_size = labels.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        running_acc += accuracy_from_logits(outputs, labels) * batch_size\n",
        "        total += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_acc / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def create_model(model_name):\n",
        "    if model_name == \"SimpleCNN\":\n",
        "        return SimpleCNN(num_classes=10)\n",
        "    elif model_name == \"DeeperCNN\":\n",
        "        return DeeperCNN(num_classes=10, dropout_p=0.5)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
        "\n",
        "def create_optimizer(optimizer_name, model_parameters, lr, weight_decay=0.0, momentum=0.9):\n",
        "    if optimizer_name == \"SGD\":\n",
        "        return optim.SGD(model_parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == \"Adam\":\n",
        "        return optim.Adam(model_parameters, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer_name: {optimizer_name}\")\n",
        "\n",
        "def create_scheduler(scheduler_name, optimizer, step_size=10, gamma=0.1):\n",
        "    if scheduler_name is None or scheduler_name.lower() == \"none\":\n",
        "        return None\n",
        "    elif scheduler_name == \"StepLR\":\n",
        "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler_name: {scheduler_name}\")\n",
        "\n",
        "def run_experiment(config):\n",
        "    \"\"\"\n",
        "    config: dict with keys:\n",
        "      - name\n",
        "      - model_name\n",
        "      - optimizer_name\n",
        "      - lr\n",
        "      - batch_size\n",
        "      - weight_decay\n",
        "      - scheduler_name\n",
        "      - num_epochs\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Starting experiment: {config['name']}\")\n",
        "    print(config)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader, val_loader, test_loader = make_dataloaders(batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # Model, loss, optimizer, scheduler\n",
        "    model = create_model(config[\"model_name\"]).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = create_optimizer(\n",
        "        optimizer_name=config[\"optimizer_name\"],\n",
        "        model_parameters=model.parameters(),\n",
        "        lr=config[\"lr\"],\n",
        "        weight_decay=config.get(\"weight_decay\", 0.0),\n",
        "        momentum=config.get(\"momentum\", 0.9)\n",
        "    )\n",
        "    scheduler = create_scheduler(\n",
        "        scheduler_name=config.get(\"scheduler_name\", None),\n",
        "        optimizer=optimizer,\n",
        "        step_size=config.get(\"step_size\", 10),\n",
        "        gamma=config.get(\"gamma\", 0.1)\n",
        "    )\n",
        "\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            train_loader=train_loader,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model=model,\n",
        "            data_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_epoch = epoch\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
        "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n",
        "    print(f\"Best Val Acc: {best_val_acc*100:.2f}% at epoch {best_epoch+1}\")\n",
        "\n",
        "    result_summary = {\n",
        "        \"experiment_name\": config[\"name\"],\n",
        "        \"model\": config[\"model_name\"],\n",
        "        \"optimizer\": config[\"optimizer_name\"],\n",
        "        \"batch_size\": config[\"batch_size\"],\n",
        "        \"lr\": config[\"lr\"],\n",
        "        \"weight_decay\": config.get(\"weight_decay\", 0.0),\n",
        "        \"scheduler\": config.get(\"scheduler_name\", \"none\"),\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"test_acc\": test_acc,\n",
        "    }\n",
        "\n",
        "    return result_summary, history\n",
        "\n",
        "def plot_learning_curves(history, title=\"Learning Curves\"):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"],   label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(title + \" - Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"],   label=\"Val Acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(title + \" - Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Qekkm5jVdu1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "NUM_EPOCHS = 15\n",
        "\n",
        "experiments = [\n",
        "    {\n",
        "        \"name\": \"SimpleCNN_SGD_bs128_nosched\",\n",
        "        \"model_name\": \"SimpleCNN\",\n",
        "        \"optimizer_name\": \"SGD\",\n",
        "        \"lr\": 0.01,\n",
        "        \"batch_size\": 128,\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"scheduler_name\": None,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"SimpleCNN_SGD_wd5e-4_stepLR\",\n",
        "        \"model_name\": \"SimpleCNN\",\n",
        "        \"optimizer_name\": \"SGD\",\n",
        "        \"lr\": 0.01,\n",
        "        \"batch_size\": 128,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"scheduler_name\": \"StepLR\",\n",
        "        \"step_size\": 3,\n",
        "        \"gamma\": 0.5,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"SimpleCNN_Adam_bs64\",\n",
        "        \"model_name\": \"SimpleCNN\",\n",
        "        \"optimizer_name\": \"Adam\",\n",
        "        \"lr\": 0.001,\n",
        "        \"batch_size\": 64,\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"scheduler_name\": None,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DeeperCNN_Adam_bs128_wd5e-4_stepLR\",\n",
        "        \"model_name\": \"DeeperCNN\",\n",
        "        \"optimizer_name\": \"Adam\",\n",
        "        \"lr\": 0.001,\n",
        "        \"batch_size\": 128,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"scheduler_name\": \"StepLR\",\n",
        "        \"step_size\": 3,\n",
        "        \"gamma\": 0.5,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "c102OjIzdywk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Experiment and results\n",
        "all_results = []\n",
        "all_histories = {}\n",
        "\n",
        "for config in experiments:\n",
        "    result_summary, history = run_experiment(config)\n",
        "    all_results.append(result_summary)\n",
        "    all_histories[config[\"name\"]] = history\n",
        "\n",
        "# Convert results for comparison\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df[\"best_val_acc_%\"] = results_df[\"best_val_acc\"] * 100\n",
        "results_df[\"test_acc_%\"] = results_df[\"test_acc\"] * 100\n",
        "\n",
        "print(\"\\n===== Summary of Experiments =====\")\n",
        "display(results_df[[\n",
        "    \"experiment_name\",\n",
        "    \"model\",\n",
        "    \"optimizer\",\n",
        "    \"batch_size\",\n",
        "    \"lr\",\n",
        "    \"weight_decay\",\n",
        "    \"scheduler\",\n",
        "    \"num_epochs\",\n",
        "    \"best_val_acc_%\",\n",
        "    \"test_acc_%\"\n",
        "]])"
      ],
      "metadata": {
        "id": "8dxfLKO_d0xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots\n",
        "for exp_name, hist in all_histories.items():\n",
        "    plot_learning_curves(hist, title=exp_name)"
      ],
      "metadata": {
        "id": "WvtuuFOod3hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}